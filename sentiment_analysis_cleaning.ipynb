{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sentiment', 'id', 'date', 'query', 'user', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 80-81: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 80-81: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-3afd568ee91a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/training.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1063\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1826\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 80-81: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/training.csv', header=None, names=cols, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=['id', 'date', 'query', 'user'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all....\n",
       "5          0                      @Kwesidei not the whole crew \n",
       "6          0                                        Need a hug \n",
       "7          0  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
       "8          0               @Tatiana_K nope they didn't have it \n",
       "9          0                          @twittera que me muera ? "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>4</td>\n",
       "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>4</td>\n",
       "      <td>im meeting up with one of my besties tonight! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>4</td>\n",
       "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>4</td>\n",
       "      <td>Being sick can be really cheap when it hurts t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800004</th>\n",
       "      <td>4</td>\n",
       "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800005</th>\n",
       "      <td>4</td>\n",
       "      <td>@ProductOfFear You can tell him that I just bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800006</th>\n",
       "      <td>4</td>\n",
       "      <td>@r_keith_hill Thans for your response. Ihad al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800007</th>\n",
       "      <td>4</td>\n",
       "      <td>@KeepinUpWKris I am so jealous, hope you had a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800008</th>\n",
       "      <td>4</td>\n",
       "      <td>@tommcfly ah, congrats mr fletcher for finally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800009</th>\n",
       "      <td>4</td>\n",
       "      <td>@e4VoIP I RESPONDED  Stupid cat is helping me ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                               text\n",
       "800000          4       I LOVE @Health4UandPets u guys r the best!! \n",
       "800001          4  im meeting up with one of my besties tonight! ...\n",
       "800002          4  @DaRealSunisaKim Thanks for the Twitter add, S...\n",
       "800003          4  Being sick can be really cheap when it hurts t...\n",
       "800004          4    @LovesBrooklyn2 he has that effect on everyone \n",
       "800005          4  @ProductOfFear You can tell him that I just bu...\n",
       "800006          4  @r_keith_hill Thans for your response. Ihad al...\n",
       "800007          4  @KeepinUpWKris I am so jealous, hope you had a...\n",
       "800008          4  @tommcfly ah, congrats mr fletcher for finally...\n",
       "800009          4  @e4VoIP I RESPONDED  Stupid cat is helping me ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.sentiment == 4].head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pre_clean_len'] =[len(df.text) for t in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>pre_clean_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  pre_clean_len\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...        1600000\n",
       "1          0  is upset that he can't update his Facebook by ...        1600000\n",
       "2          0  @Kenichan I dived many times for the ball. Man...        1600000\n",
       "3          0    my whole body feels itchy and like its on fire         1600000\n",
       "4          0  @nationwideclass no, it's not behaving at all....        1600000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'sentiment': {\n",
    "        'type': df.sentiment.dtype,\n",
    "        'description': 'sentiment class; 0-negative and 4-positive'\n",
    "    },\n",
    "    'text': {\n",
    "        'type': df.text.dtype,\n",
    "        'description': 'tweet text'\n",
    "    },\n",
    "    'pre_clean_len': {\n",
    "        'type': df.pre_clean_len.dtype,\n",
    "        'description': 'length of a tweer text before cleaning'\n",
    "    },\n",
    "    'dataset_shape': df.shape\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_shape': (1600000, 3),\n",
      " 'pre_clean_len': {'description': 'length of a tweer text before cleaning',\n",
      "                   'type': dtype('int64')},\n",
      " 'sentiment': {'description': 'sentiment class; 0-negative and 4-positive',\n",
      "               'type': dtype('int64')},\n",
      " 'text': {'description': 'tweet text', 'type': dtype('O')}}\n"
     ]
    }
   ],
   "source": [
    "pprint(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125a6e5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(df.pre_clean_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>pre_clean_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Need a hug</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  pre_clean_len\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...        1600000\n",
       "1          0  is upset that he can't update his Facebook by ...        1600000\n",
       "2          0  @Kenichan I dived many times for the ball. Man...        1600000\n",
       "3          0    my whole body feels itchy and like its on fire         1600000\n",
       "4          0  @nationwideclass no, it's not behaving at all....        1600000\n",
       "5          0                      @Kwesidei not the whole crew         1600000\n",
       "6          0                                        Need a hug         1600000\n",
       "7          0  @LOLTrish hey  long time no see! Yes.. Rains a...        1600000\n",
       "8          0               @Tatiana_K nope they didn't have it         1600000\n",
       "9          0                          @twittera que me muera ?         1600000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.pre_clean_len > 140].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see, some comments have still undecoded HTML strings. We can handle that by using BeautifulSoup lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text.loc[343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@TheLeagueSF Not Fun &amp; Furious? The new mantra for the Bay 2 Breakers? It was getting 2 rambunctious;the city overreacted &amp; clamped down \n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = BeautifulSoup(text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = processed_text.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with mentioning (@)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Not Fun & Furious? The new mantra for the Bay 2 Breakers? It was getting 2 rambunctious;the city overreacted & clamped down '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(r'@[A-Za-z0-0]+','',processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_text = re.sub(r'https?://[A-Za-z0-9./]+', '', df.text.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot  - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTF-8 BOM (Byte Order Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bom_text = df.text[226]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tuesdayll start with reflection n then a lecture in Stress reducing techniques That sure might become very useful for us accompaniers '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(list(filter(lambda x: x.encode('utf8').isalpha() or x.isdigit() or x==' ', bom_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove neither alphabetic nor numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@machineplay I'm so sorry you're having to go through this. Again.  #therapyfail\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' machineplay I m so sorry you re having to go through this  Again    therapyfail'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[^A-Za-z0-9]', ' ', df.text[175])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanning function (summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@[A-Za-z0-9]+|https?://[A-Za-z0-9./]+\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "pattern_1 = r'@[A-Za-z0-9]+'\n",
    "pattern_2 = r'https?://[A-Za-z0-9./]+'\n",
    "pattern = r'|'.join([pattern_1, pattern_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(pattern, '', souped)\n",
    "    try:\n",
    "        cleaned = ''.join(list(\n",
    "                            filter(\n",
    "                                lambda x: x.encode('utf8').isalpha() or x.isdigit() or x==' ',\n",
    "                                stripped\n",
    "                            )))\n",
    "    except:\n",
    "        cleaned = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", cleaned)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tokenizer.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awww thats a bummer you shoulda got david carr of third day to do it d', 'is upset that he cant update his facebook by texting it and might cry as a result school today also blah', 'i dived many times for the ball managed to save the rest go out of bounds', 'my whole body feels itchy and like its on fire', 'no its not behaving at all im mad why am i here because i cant see you all over there', 'not the whole crew', 'need a hug', 'hey long time no see yes rains a bit only a bit lol im fine thanks hows you', 'k nope they didnt have it', 'que me muera', 'spring break in plain city its snowing', 'i just repierced my ears', 'i couldnt bear to watch it and i thought the ua loss was embarrassing', 'it it counts idk why i did either you never talk to me anymore', 'i wouldve been the first but i didnt have a gun not really though zac snyders just a doucheclown', 'i wish i got to watch it with you i miss you and how was the premiere', 'hollis death scene will hurt me severely to watch on film wry is directors cut not out now', 'about to file taxes', 'ahh ive always wanted to see rent love the soundtrack', 'oh dear were you drinking out of the forgotten table drinks', 'i was out most of the day so didnt get much done', 'one of my friend called me and asked to meet with her at mid valley todaybut ive no time sigh', 'barista i baked you a cake but i ated it', 'this week is not going as i had hoped', 'blagh class at tomorrow', 'i hate when i have to call and wake people up', 'just going to cry myself to sleep after watching marley and me', 'im sad now misslilly', 'ooooh lol that leslie and ok i wont do it again so leslie wont get mad again', 'meh almost lover is the exception this track gets me depressed every time', 'some hacked my account on aim now i have to make a new one', 'i want to go to promote gear and groove but unfornately no ride there i may b going to the one in anaheim in may though', 'thought sleeping in was an option tomorrow but realizing that it now is not evaluations in the morning and work in the afternoon', 'awe i love you too am here i miss you', 'i cry my asian eyes to sleep at night', 'ok im sick and spent an hour sitting in the shower cause i was too sick to stand and held back the puke like a champ bed now', 'ill tell ya the story later not a good day and ill be workin for like three more hours', 'sorry bed time came here gmt', 'i dont either its depressing i dont think i even want to know about the kids in suitcases', 'bed class work gym or then class another day thats gonna fly by i miss my girlfriend', 'really dont feel like getting up today but got to study to for tomorrows practical exam', 'hes the reason for the teardrops on my guitar the only one who has enough of me to break my heart', 'sad sad sad i dont know why but i hate this feeling i wanna sleep and i still cant', 'awww i soo wish i was there to see you finally comfortable im sad that i missed it', 'falling asleep just heard about that tracy girls body being found how sad my heart breaks for that family', 'yay im happy for you with your job but that also means less time for me and you', 'just checked my user timeline on my blackberry it looks like the twanking is still happening are ppl still having probs w bgs and uids', 'oh manwas ironing s fave top to wear to a meeting burnt it', 'is strangely sad about lilo and samro breaking up', 'oh im so sorry i didnt think about that before retweeting', 'broadband plan a massive broken promise via wwwdiigocomtautao still waiting for broadband we are', 'wow tons of replies from you may have to unfollow so i can see my friends tweets youre scrolling the feed a lot', 'our duck and chicken are taking wayyy too long to hatch', 'put vacation photos online a few yrs ago pc crashed and now i forget the name of the site', 'i need a hug', 'not sure what they are only that they are pos as much as i want to i dont think can trade away company assets sorry andy', 'i hate when that happens', 'i have a sad feeling that dallas is not going to show up i gotta say though youd think more shows would use music from the game mmm', 'ugh degrees tomorrow', 'where did u move to i thought u were already in sd hmmm random u found me glad to hear yer doing well', 'i miss my ps its out of commission wutcha playing have you copped blood on the sand', 'just leaving the parking lot of work', 'the life is cool but not for me', 'sadly though ive never gotten to experience the post coitus cigarette before and now i never will', 'i had such a nice day too bad the rain comes in tomorrow at am', 'too bad i wont be around i lost my job and cant even pay my phone bill lmao aw shucks', 'damm back to school tomorrow', 'mo jobs no money how in the hell is min wage here fn clams an hour', 'not forever see you soon', 'algonquin agreed i saw the failwhale allllll day today', 'oh haha dude i dont really look at em unless someone says hey i added you sorry im so terrible at that i need a pop up', 'im sure youre right i need to start working out with you and the nikster or jared at least', 'i really hate how people diss my bands trace is clearly not ugly', 'gym attire today was puma singlet adidas shortsand black business socks and leather shoes lucky did not run into any cute girls', 'why wont you show my location', 'no picnic my phone smells like citrus', 'my donkey is sensitive about such comments nevertheless hed and med be glad to see your mug asap charger is still awol', 'no new csi tonight fml', 'i think my arms are sore from tennis', 'wonders why someone that u like so much can make you so unhappy in a split seccond depressed', 'sleep soon i just hate saying bye and see you tomorrow for the night', 'just got ur newsletter those fares really are unbelievable shame i already booked and paid for mine', 'missin the boo', 'me too itm', 'damn i dont have any chalk my chalkboard is useless', 'had a blast at the getty villa but hates that shes had a sore throat all day its just getting worse too', 'hey missed ya at the meeting sup mama', 'my tummy hurts i wonder if the hypnosis has anything to do with it if so its working i get it stop smoking', 'why is it always the fat ones', 'sorry babe my fam annoys me too thankfully theyre asleep right now muahaha evil laugh', 'i should have paid more attention when we covered photoshop in my webpage design class in undergrad', 'wednesday my bday dont know what do', 'poor cameron the hills', 'pray for me please the ex is threatening to start sh at myour babies st birthday party what a jerk and i still have a headache', 'hmm do u really enjoy being with him if the problems are too constants u should think things more find someone ulike', 'strider is a sick little puppy', 'so ryleegracewana go steves party or not sadly since its easter i wnt b able do much but ohh well', 'hey i actually won one of my bracket pools too bad it wasnt the one for money', 'you dont follow me either and i work for you', 'a bad nite for the favorite teams astros and spartans lose the nite out with tw was good']\n"
     ]
    }
   ],
   "source": [
    "testing = df.text[:100]\n",
    "test_results = []\n",
    "for testing_tweet in testing:\n",
    "    test_results.append(\n",
    "        cleaner(testing_tweet)\n",
    "    )\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing tweets\n"
     ]
    }
   ],
   "source": [
    "nums = [0, 400000, 800000, 1200000, 1600000]\n",
    "ranges = list(zip(range(len(nums) - 1), range(1,len(nums))))\n",
    "print('Cleaning and parsing tweets')\n",
    "clean_tweet_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets 10000 of 400000 have been processed\n",
      "Tweets 20000 of 400000 have been processed\n",
      "Tweets 30000 of 400000 have been processed\n",
      "Tweets 40000 of 400000 have been processed\n",
      "Tweets 50000 of 400000 have been processed\n",
      "Tweets 60000 of 400000 have been processed\n",
      "Tweets 70000 of 400000 have been processed\n",
      "Tweets 80000 of 400000 have been processed\n",
      "Tweets 90000 of 400000 have been processed\n",
      "Tweets 100000 of 400000 have been processed\n",
      "Tweets 110000 of 400000 have been processed\n",
      "Tweets 120000 of 400000 have been processed\n",
      "Tweets 130000 of 400000 have been processed\n",
      "Tweets 140000 of 400000 have been processed\n",
      "Tweets 150000 of 400000 have been processed\n",
      "Tweets 160000 of 400000 have been processed\n",
      "Tweets 170000 of 400000 have been processed\n",
      "Tweets 180000 of 400000 have been processed\n",
      "Tweets 190000 of 400000 have been processed\n",
      "Tweets 200000 of 400000 have been processed\n",
      "Tweets 210000 of 400000 have been processed\n",
      "Tweets 220000 of 400000 have been processed\n",
      "Tweets 230000 of 400000 have been processed\n",
      "Tweets 240000 of 400000 have been processed\n",
      "Tweets 250000 of 400000 have been processed\n",
      "Tweets 260000 of 400000 have been processed\n",
      "Tweets 270000 of 400000 have been processed\n",
      "Tweets 280000 of 400000 have been processed\n",
      "Tweets 290000 of 400000 have been processed\n",
      "Tweets 300000 of 400000 have been processed\n",
      "Tweets 310000 of 400000 have been processed\n",
      "Tweets 320000 of 400000 have been processed\n",
      "Tweets 330000 of 400000 have been processed\n",
      "Tweets 340000 of 400000 have been processed\n",
      "Tweets 350000 of 400000 have been processed\n",
      "Tweets 360000 of 400000 have been processed\n",
      "Tweets 370000 of 400000 have been processed\n",
      "Tweets 380000 of 400000 have been processed\n",
      "Tweets 390000 of 400000 have been processed\n",
      "Tweets 400000 of 400000 have been processed\n",
      "Tweets 410000 of 800000 have been processed\n",
      "Tweets 420000 of 800000 have been processed\n",
      "Tweets 430000 of 800000 have been processed\n",
      "Tweets 440000 of 800000 have been processed\n",
      "Tweets 450000 of 800000 have been processed\n",
      "Tweets 460000 of 800000 have been processed\n",
      "Tweets 470000 of 800000 have been processed\n",
      "Tweets 480000 of 800000 have been processed\n",
      "Tweets 490000 of 800000 have been processed\n",
      "Tweets 500000 of 800000 have been processed\n",
      "Tweets 510000 of 800000 have been processed\n",
      "Tweets 520000 of 800000 have been processed\n",
      "Tweets 530000 of 800000 have been processed\n",
      "Tweets 540000 of 800000 have been processed\n",
      "Tweets 550000 of 800000 have been processed\n",
      "Tweets 560000 of 800000 have been processed\n",
      "Tweets 570000 of 800000 have been processed\n",
      "Tweets 580000 of 800000 have been processed\n",
      "Tweets 590000 of 800000 have been processed\n",
      "Tweets 600000 of 800000 have been processed\n",
      "Tweets 610000 of 800000 have been processed\n",
      "Tweets 620000 of 800000 have been processed\n",
      "Tweets 630000 of 800000 have been processed\n",
      "Tweets 640000 of 800000 have been processed\n",
      "Tweets 650000 of 800000 have been processed\n",
      "Tweets 660000 of 800000 have been processed\n",
      "Tweets 670000 of 800000 have been processed\n",
      "Tweets 680000 of 800000 have been processed\n",
      "Tweets 690000 of 800000 have been processed\n",
      "Tweets 700000 of 800000 have been processed\n",
      "Tweets 710000 of 800000 have been processed\n",
      "Tweets 720000 of 800000 have been processed\n",
      "Tweets 730000 of 800000 have been processed\n",
      "Tweets 740000 of 800000 have been processed\n",
      "Tweets 750000 of 800000 have been processed\n",
      "Tweets 760000 of 800000 have been processed\n",
      "Tweets 770000 of 800000 have been processed\n",
      "Tweets 780000 of 800000 have been processed\n",
      "Tweets 790000 of 800000 have been processed\n",
      "Tweets 800000 of 800000 have been processed\n",
      "Tweets 810000 of 1200000 have been processed\n",
      "Tweets 820000 of 1200000 have been processed\n",
      "Tweets 830000 of 1200000 have been processed\n",
      "Tweets 840000 of 1200000 have been processed\n",
      "Tweets 850000 of 1200000 have been processed\n",
      "Tweets 860000 of 1200000 have been processed\n",
      "Tweets 870000 of 1200000 have been processed\n",
      "Tweets 880000 of 1200000 have been processed\n",
      "Tweets 890000 of 1200000 have been processed\n",
      "Tweets 900000 of 1200000 have been processed\n",
      "Tweets 910000 of 1200000 have been processed\n",
      "Tweets 920000 of 1200000 have been processed\n",
      "Tweets 930000 of 1200000 have been processed\n",
      "Tweets 940000 of 1200000 have been processed\n",
      "Tweets 950000 of 1200000 have been processed\n",
      "Tweets 960000 of 1200000 have been processed\n",
      "Tweets 970000 of 1200000 have been processed\n",
      "Tweets 980000 of 1200000 have been processed\n",
      "Tweets 990000 of 1200000 have been processed\n",
      "Tweets 1000000 of 1200000 have been processed\n",
      "Tweets 1010000 of 1200000 have been processed\n",
      "Tweets 1020000 of 1200000 have been processed\n",
      "Tweets 1030000 of 1200000 have been processed\n",
      "Tweets 1040000 of 1200000 have been processed\n",
      "Tweets 1050000 of 1200000 have been processed\n",
      "Tweets 1060000 of 1200000 have been processed\n",
      "Tweets 1070000 of 1200000 have been processed\n",
      "Tweets 1080000 of 1200000 have been processed\n",
      "Tweets 1090000 of 1200000 have been processed\n",
      "Tweets 1100000 of 1200000 have been processed\n",
      "Tweets 1110000 of 1200000 have been processed\n",
      "Tweets 1120000 of 1200000 have been processed\n",
      "Tweets 1130000 of 1200000 have been processed\n",
      "Tweets 1140000 of 1200000 have been processed\n",
      "Tweets 1150000 of 1200000 have been processed\n",
      "Tweets 1160000 of 1200000 have been processed\n",
      "Tweets 1170000 of 1200000 have been processed\n",
      "Tweets 1180000 of 1200000 have been processed\n",
      "Tweets 1190000 of 1200000 have been processed\n",
      "Tweets 1200000 of 1200000 have been processed\n",
      "Tweets 1210000 of 1600000 have been processed\n",
      "Tweets 1220000 of 1600000 have been processed\n",
      "Tweets 1230000 of 1600000 have been processed\n",
      "Tweets 1240000 of 1600000 have been processed\n",
      "Tweets 1250000 of 1600000 have been processed\n",
      "Tweets 1260000 of 1600000 have been processed\n",
      "Tweets 1270000 of 1600000 have been processed\n",
      "Tweets 1280000 of 1600000 have been processed\n",
      "Tweets 1290000 of 1600000 have been processed\n",
      "Tweets 1300000 of 1600000 have been processed\n",
      "Tweets 1310000 of 1600000 have been processed\n",
      "Tweets 1320000 of 1600000 have been processed\n",
      "Tweets 1330000 of 1600000 have been processed\n",
      "Tweets 1340000 of 1600000 have been processed\n",
      "Tweets 1350000 of 1600000 have been processed\n",
      "Tweets 1360000 of 1600000 have been processed\n",
      "Tweets 1370000 of 1600000 have been processed\n",
      "Tweets 1380000 of 1600000 have been processed\n",
      "Tweets 1390000 of 1600000 have been processed\n",
      "Tweets 1400000 of 1600000 have been processed\n",
      "Tweets 1410000 of 1600000 have been processed\n",
      "Tweets 1420000 of 1600000 have been processed\n",
      "Tweets 1430000 of 1600000 have been processed\n",
      "Tweets 1440000 of 1600000 have been processed\n",
      "Tweets 1450000 of 1600000 have been processed\n",
      "Tweets 1460000 of 1600000 have been processed\n",
      "Tweets 1470000 of 1600000 have been processed\n",
      "Tweets 1480000 of 1600000 have been processed\n",
      "Tweets 1490000 of 1600000 have been processed\n",
      "Tweets 1500000 of 1600000 have been processed\n",
      "Tweets 1510000 of 1600000 have been processed\n",
      "Tweets 1520000 of 1600000 have been processed\n",
      "Tweets 1530000 of 1600000 have been processed\n",
      "Tweets 1540000 of 1600000 have been processed\n",
      "Tweets 1550000 of 1600000 have been processed\n",
      "Tweets 1560000 of 1600000 have been processed\n",
      "Tweets 1570000 of 1600000 have been processed\n",
      "Tweets 1580000 of 1600000 have been processed\n",
      "Tweets 1590000 of 1600000 have been processed\n",
      "Tweets 1600000 of 1600000 have been processed\n"
     ]
    }
   ],
   "source": [
    "for start, end in ranges:\n",
    "    for i in range(nums[start], nums[end]):\n",
    "        \n",
    "        if (i+1) % 10000 == 0 :\n",
    "            print('Tweets {0} of {1} have been processed'.format(i + 1, nums[end]))\n",
    "        \n",
    "        clean_tweet_text.append(\n",
    "            cleaner(df.text[i])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awww thats a bummer you shoulda got david carr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  awww thats a bummer you shoulda got david carr...     0.0\n",
       "1  is upset that he cant update his facebook by t...     0.0\n",
       "2  i dived many times for the ball managed to sav...     0.0\n",
       "3     my whole body feels itchy and like its on fire     0.0\n",
       "4  no its not behaving at all im mad why am i her...     0.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = pd.DataFrame(clean_tweet_text, columns=['text'])\n",
    "cleaned_df['target'] = df.sentiment\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'clean_tweets.csv'\n",
    "cleaned_df."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
